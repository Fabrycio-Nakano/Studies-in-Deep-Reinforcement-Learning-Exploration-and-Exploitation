{
    "name": "root",
    "gauges": {
        "BigWallJump.Policy.Entropy.mean": {
            "value": 1.9436436891555786,
            "min": 1.9436436891555786,
            "max": 3.735485315322876,
            "count": 9
        },
        "BigWallJump.Policy.Entropy.sum": {
            "value": 38717.3828125,
            "min": 38717.3828125,
            "max": 79270.734375,
            "count": 9
        },
        "BigWallJump.Environment.EpisodeLength.mean": {
            "value": 160.3658536585366,
            "min": 83.2433628318584,
            "max": 190.05454545454546,
            "count": 9
        },
        "BigWallJump.Environment.EpisodeLength.sum": {
            "value": 19725.0,
            "min": 18714.0,
            "max": 20906.0,
            "count": 9
        },
        "BigWallJump.Step.mean": {
            "value": 179953.0,
            "min": 19935.0,
            "max": 179953.0,
            "count": 9
        },
        "BigWallJump.Step.sum": {
            "value": 179953.0,
            "min": 19935.0,
            "max": 179953.0,
            "count": 9
        },
        "BigWallJump.Policy.ExtrinsicValue.mean": {
            "value": 0.7645332217216492,
            "min": 0.2939241826534271,
            "max": 0.8215134739875793,
            "count": 9
        },
        "BigWallJump.Policy.ExtrinsicValue.sum": {
            "value": 170.49090576171875,
            "min": 90.23472595214844,
            "max": 184.05795288085938,
            "count": 9
        },
        "BigWallJump.Environment.CumulativeReward.mean": {
            "value": -1.1897195334114679,
            "min": -1.2936000298031352,
            "max": -1.1462101884571572,
            "count": 9
        },
        "BigWallJump.Environment.CumulativeReward.sum": {
            "value": -146.33550260961056,
            "min": -259.0435025913175,
            "max": -137.05450358241796,
            "count": 9
        },
        "BigWallJump.Policy.ExtrinsicReward.mean": {
            "value": -1.1897195334114679,
            "min": -1.2936000298031352,
            "max": -1.1462101884571572,
            "count": 9
        },
        "BigWallJump.Policy.ExtrinsicReward.sum": {
            "value": -146.33550260961056,
            "min": -259.0435025913175,
            "max": -137.05450358241796,
            "count": 9
        },
        "BigWallJump.Losses.PolicyLoss.mean": {
            "value": -3.0591230945177625,
            "min": -3.249406656086335,
            "max": -1.296950123702181,
            "count": 9
        },
        "BigWallJump.Losses.PolicyLoss.sum": {
            "value": -3037.709232856138,
            "min": -3239.658436118076,
            "max": -1291.7623232073722,
            "count": 9
        },
        "BigWallJump.Losses.ValueLoss.mean": {
            "value": 0.0005807696737897513,
            "min": 0.0005188843612657131,
            "max": 0.0015241428759606727,
            "count": 9
        },
        "BigWallJump.Losses.ValueLoss.sum": {
            "value": 0.576704286073223,
            "min": 0.520959898710776,
            "max": 1.51804630445683,
            "count": 9
        },
        "BigWallJump.Losses.Q1Loss.mean": {
            "value": 0.008350778248584717,
            "min": 0.008350778248584717,
            "max": 0.01315721890074056,
            "count": 9
        },
        "BigWallJump.Losses.Q1Loss.sum": {
            "value": 8.292322800844623,
            "min": 8.292322800844623,
            "max": 13.183533338542041,
            "count": 9
        },
        "BigWallJump.Losses.Q2Loss.mean": {
            "value": 0.008389927255227565,
            "min": 0.008389927255227565,
            "max": 0.013117952738408949,
            "count": 9
        },
        "BigWallJump.Losses.Q2Loss.sum": {
            "value": 8.331197764440972,
            "min": 8.331197764440972,
            "max": 13.144188643885768,
            "count": 9
        },
        "BigWallJump.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.010366213295003904,
            "min": 0.010366213295003904,
            "max": 0.08649163710082974,
            "count": 9
        },
        "BigWallJump.Policy.DiscreteEntropyCoeff.sum": {
            "value": 10.293649801938876,
            "min": 10.293649801938876,
            "max": 86.14567055242642,
            "count": 9
        },
        "BigWallJump.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.09999999403953552,
            "min": 0.09999999403953552,
            "max": 0.09999999403953552,
            "count": 9
        },
        "BigWallJump.Policy.ContinuousEntropyCoeff.sum": {
            "value": 99.29999408125877,
            "min": 99.29999408125877,
            "max": 100.39999401569366,
            "count": 9
        },
        "BigWallJump.Policy.LearningRate.mean": {
            "value": 0.0002999999999999999,
            "min": 0.0002999999999999999,
            "max": 0.00030000000000000003,
            "count": 9
        },
        "BigWallJump.Policy.LearningRate.sum": {
            "value": 0.29789999999999994,
            "min": 0.29789999999999994,
            "max": 0.30119999999999997,
            "count": 9
        },
        "BigWallJump.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 9
        },
        "BigWallJump.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 9
        },
        "SmallWallJump.Policy.Entropy.mean": {
            "value": 3.112377166748047,
            "min": 3.112377166748047,
            "max": 3.7216689586639404,
            "count": 2
        },
        "SmallWallJump.Policy.Entropy.sum": {
            "value": 62776.6484375,
            "min": 62776.6484375,
            "max": 75073.5078125,
            "count": 2
        },
        "SmallWallJump.Environment.EpisodeLength.mean": {
            "value": 58.30473372781065,
            "min": 48.174447174447174,
            "max": 58.30473372781065,
            "count": 2
        },
        "SmallWallJump.Environment.EpisodeLength.sum": {
            "value": 19707.0,
            "min": 19607.0,
            "max": 19707.0,
            "count": 2
        },
        "SmallWallJump.Step.mean": {
            "value": 39981.0,
            "min": 19968.0,
            "max": 39981.0,
            "count": 2
        },
        "SmallWallJump.Step.sum": {
            "value": 39981.0,
            "min": 19968.0,
            "max": 39981.0,
            "count": 2
        },
        "SmallWallJump.Policy.ExtrinsicValue.mean": {
            "value": 0.2882767617702484,
            "min": -0.006946283858269453,
            "max": 0.2882767617702484,
            "count": 2
        },
        "SmallWallJump.Policy.ExtrinsicValue.sum": {
            "value": 110.69827270507812,
            "min": -3.1258277893066406,
            "max": 110.69827270507812,
            "count": 2
        },
        "SmallWallJump.Environment.CumulativeReward.mean": {
            "value": -0.870383143623376,
            "min": -1.0120911403744668,
            "max": -0.870383143623376,
            "count": 2
        },
        "SmallWallJump.Environment.CumulativeReward.sum": {
            "value": -294.1895025447011,
            "min": -410.9090029920335,
            "max": -294.1895025447011,
            "count": 2
        },
        "SmallWallJump.Policy.ExtrinsicReward.mean": {
            "value": -0.870383143623376,
            "min": -1.0120911403744668,
            "max": -0.870383143623376,
            "count": 2
        },
        "SmallWallJump.Policy.ExtrinsicReward.sum": {
            "value": -294.1895025447011,
            "min": -410.9090029920335,
            "max": -294.1895025447011,
            "count": 2
        },
        "SmallWallJump.Losses.PolicyLoss.mean": {
            "value": -1.0581913300084345,
            "min": -1.0581913300084345,
            "max": 0.050452427266352975,
            "count": 2
        },
        "SmallWallJump.Losses.PolicyLoss.sum": {
            "value": -1059.249521338443,
            "min": -1059.249521338443,
            "max": 50.35152241182027,
            "count": 2
        },
        "SmallWallJump.Losses.ValueLoss.mean": {
            "value": 0.0011401499617983532,
            "min": 0.0011401499617983532,
            "max": 0.0014046978382082638,
            "count": 2
        },
        "SmallWallJump.Losses.ValueLoss.sum": {
            "value": 1.1412901117601515,
            "min": 1.1412901117601515,
            "max": 1.4018884425318472,
            "count": 2
        },
        "SmallWallJump.Losses.Q1Loss.mean": {
            "value": 0.01168753334505867,
            "min": 0.008139694767797396,
            "max": 0.01168753334505867,
            "count": 2
        },
        "SmallWallJump.Losses.Q1Loss.sum": {
            "value": 11.699220878403729,
            "min": 8.123415378261802,
            "max": 11.699220878403729,
            "count": 2
        },
        "SmallWallJump.Losses.Q2Loss.mean": {
            "value": 0.011703974057787205,
            "min": 0.008352127639108769,
            "max": 0.011703974057787205,
            "count": 2
        },
        "SmallWallJump.Losses.Q2Loss.sum": {
            "value": 11.715678031844991,
            "min": 8.335423383830552,
            "max": 11.715678031844991,
            "count": 2
        },
        "SmallWallJump.Policy.DiscreteEntropyCoeff.mean": {
            "value": 0.06600330340443013,
            "min": 0.06600330340443013,
            "max": 0.0866329222206422,
            "count": 2
        },
        "SmallWallJump.Policy.DiscreteEntropyCoeff.sum": {
            "value": 66.06930670783456,
            "min": 66.06930670783456,
            "max": 86.45965637620091,
            "count": 2
        },
        "SmallWallJump.Policy.ContinuousEntropyCoeff.mean": {
            "value": 0.09999999403953552,
            "min": 0.09999999403953552,
            "max": 0.09999999403953552,
            "count": 2
        },
        "SmallWallJump.Policy.ContinuousEntropyCoeff.sum": {
            "value": 100.09999403357506,
            "min": 99.79999405145645,
            "max": 100.09999403357506,
            "count": 2
        },
        "SmallWallJump.Policy.LearningRate.mean": {
            "value": 0.00030000000000000003,
            "min": 0.0003,
            "max": 0.00030000000000000003,
            "count": 2
        },
        "SmallWallJump.Policy.LearningRate.sum": {
            "value": 0.3003,
            "min": 0.2994,
            "max": 0.3003,
            "count": 2
        },
        "SmallWallJump.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        },
        "SmallWallJump.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 2
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1732732721",
        "python_version": "3.9.20 (main, Oct  3 2024, 07:38:01) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "C:\\Users\\fabry\\anaconda3\\envs\\mlagents20\\Scripts\\mlagents-learn config/sac/WallJump.yaml --run-id=testing_walljump.sac",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.5.1+cpu",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1732734085"
    },
    "total": 1364.6603904,
    "count": 1,
    "self": 0.03097420000017337,
    "children": {
        "run_training.setup": {
            "total": 0.14718969999999976,
            "count": 1,
            "self": 0.14718969999999976
        },
        "TrainerController.start_learning": {
            "total": 1364.4822265,
            "count": 1,
            "self": 0.5152676999980486,
            "children": {
                "TrainerController._reset_env": {
                    "total": 11.528543200000001,
                    "count": 1,
                    "self": 11.528543200000001
                },
                "TrainerController.advance": {
                    "total": 1351.608736400002,
                    "count": 12085,
                    "self": 0.5588772999938101,
                    "children": {
                        "env_step": {
                            "total": 411.9961504000021,
                            "count": 12085,
                            "self": 335.94256620001704,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 75.77578749998861,
                                    "count": 12085,
                                    "self": 2.488674499981528,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 73.28711300000708,
                                            "count": 20494,
                                            "self": 73.28711300000708
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.2777966999964132,
                                    "count": 12084,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 1353.7916681000072,
                                            "count": 12084,
                                            "is_parallel": true,
                                            "self": 1057.554710600009,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.00911220000000057,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0015513000000026977,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.007560899999997872,
                                                            "count": 12,
                                                            "is_parallel": true,
                                                            "self": 0.007560899999997872
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 296.22784529999814,
                                                    "count": 12084,
                                                    "is_parallel": true,
                                                    "self": 12.96549530001829,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 8.179617099995498,
                                                            "count": 12084,
                                                            "is_parallel": true,
                                                            "self": 8.179617099995498
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 231.20714349999514,
                                                            "count": 12084,
                                                            "is_parallel": true,
                                                            "self": 231.20714349999514
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 43.875589399989195,
                                                            "count": 24168,
                                                            "is_parallel": true,
                                                            "self": 8.277745999986642,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 35.59784340000255,
                                                                    "count": 145008,
                                                                    "is_parallel": true,
                                                                    "self": 35.59784340000255
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 939.0537087000062,
                            "count": 24168,
                            "self": 1.7101627000190547,
                            "children": {
                                "process_trajectory": {
                                    "total": 33.36571609999458,
                                    "count": 24168,
                                    "self": 33.36571609999458
                                },
                                "_update_policy": {
                                    "total": 903.9778298999926,
                                    "count": 24059,
                                    "self": 0.34251330000313374,
                                    "children": {
                                        "OffPolicyTrainer._update_policy": {
                                            "total": 903.6353165999894,
                                            "count": 24059,
                                            "self": 372.1455906999943,
                                            "children": {
                                                "TorchSACOptimizer.update": {
                                                    "total": 531.4897258999952,
                                                    "count": 12246,
                                                    "self": 531.4897258999952
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.4999998256826075e-06,
                    "count": 1,
                    "self": 1.4999998256826075e-06
                },
                "TrainerController._save_models": {
                    "total": 0.8296777000000475,
                    "count": 1,
                    "self": 0.06199149999997644,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.767686200000071,
                            "count": 2,
                            "self": 0.767686200000071
                        }
                    }
                }
            }
        }
    }
}